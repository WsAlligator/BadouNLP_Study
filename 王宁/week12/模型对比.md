| 名字/项目 | 位置编码 | transformer结构 | 多头机制 | ff层设计 | 归一化层选择 | 激活函数 | 是否使用bias |
|---|---|---|---|---|---|---|---|
| **LLaMA 3** | RoPE (旋转位置编码) | Decoder-Only | GQA (Grouped-Query Attention) | SwiGLU FFN (2层Linear + SwiGLU) | Pre-RMSNorm | SwiGLU | 否 (Attention与FFN均去bias) [^45^] |
| **Qwen 2.5** | RoPE | Decoder-Only | MHA + GQA (部分层) | SwiGLU FFN | Pre-RMSNorm | SwiGLU | 否 (Attention去bias，FFN保留bias) |
| **DeepSeek-V3** | RoPE | Decoder-Only (交替Dense/MoE) | MLA (Multi-Head Latent Attention) | MoE-SwiGLU (8专家，top-2路由) | Pre-RMSNorm | SwiGLU | 否 |
| **Mistral-Large 2** | RoPE | Decoder-Only | GQA + Sliding-Window Attention | SwiGLU FFN | Pre-RMSNorm | SwiGLU | 否 |
| **Llama 2** | RoPE | Decoder-Only | GQA | SwiGLU FFN | Pre-RMSNorm | SwiGLU | 否 |
| **Falcon-180B** | RoPE | Decoder-Only | MHA (并行注意力) | SwiGLU FFN | Pre-RMSNorm | SwiGLU | 是 |
| **Baichuan 2** | ALiBi | Decoder-Only | MHA | SwiGLU FFN | Pre-RMSNorm | SwiGLU | 是 |
| **InternLM-20B** | RoPE | Decoder-Only | MHA | SwiGLU FFN | Pre-RMSNorm | SwiGLU | 否 |
| **Yi-34B** | RoPE | Decoder-Only | GQA | SwiGLU FFN | Pre-RMSNorm | SwiGLU | 否 |
| **OPT-66B** | Learnable Pos Embed | Decoder-Only | MHA | ReLU FFN (2层Linear + ReLU) | Post-LN | ReLU | 是  |
