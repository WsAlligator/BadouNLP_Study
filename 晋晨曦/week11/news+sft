#coding:utf8
import json

import torch
import torch.nn as nn
import numpy as np
import math
import random
import os
import re
from torch.utils.data import Dataset, DataLoader
from transformers import BertTokenizer,BertModel
"""
基于pytorch的LSTM语言模型
"""

#SFT（Supervised Fine-Tuning，有监督微调）
class LanguageModel(nn.Module):
    def __init__(self,bert_path):
        super(LanguageModel, self).__init__()
        self.encoder=BertModel.from_pretrained(bert_path,return_dict=False)
        self.classify = nn.Linear(768, 21128)
        self.loss = nn.CrossEntropyLoss(ignore_index=-1)

    #当输入真实标签，返回loss值；无真实标签，返回预测值
    def forward(self, x, y=None,mask=None):
        if mask is not None:
            x, _ = self.encoder(x, attention_mask=mask)
            y_pred = self.classify(x)   #output shape:(batch_size, vocab_size)
            return self.loss(y_pred.view(-1, y_pred.shape[-1]), y.view(-1))
        else:
            #预测时，可以不使用mask
            x, _ = self.encoder(x)
            y_pred = self.classify(x)   #output shape:(batch_size, vocab_size)
            return torch.softmax(y_pred, dim=-1)

#加载标题和文章
def load_corpus(path,tokenizer):
    corpus=[]
    with open(path, encoding="utf-8") as f:
        for line in f:
            line=json.loads(line)
            title = line["title"]
            content = line["content"]
            corpus.append([title,content])
    return corpus

def build_dataset(sample_length, corpus,model,toki,max_length):
    dataset=[]
    for title, content in corpus:
        title=toki.encode(title,add_special_tokens=False)
        content=toki.encode(content,add_special_tokens=False)
        x=[toki.cls_token_id]+title+[toki.sep_token_id]+content+[toki.sep_token_id]
        y=[-1]*(len(title)+1)+content+[toki.sep_token_id]+[-1]
        mask=SFT_mask(len(title),len(content))
        x=x[:max_length]+[0]*(max_length-len(x))
        y=y[:max_length]+[0]*(max_length-len(y))
        x=torch.LongTensor(x)
        y=torch.LongTensor(y)
        mask=padmask(mask,max_length)
        dataset.append([x,y,mask])
    return DataLoader(dataset,batch_size=sample_length,shuffle=True)

def SFT_mask(length,width):
    matrix_len=length+width+3
    mask=torch.tril(torch.ones((matrix_len,matrix_len)))
    for i in range(mask.shape[0]):
        for j in range(mask.shape[1]):
            if j<=length+1:
                mask[i,j]=1
            else:break
    return mask

def padmask(mask, max_length):
    padding_matrixs=torch.zeros(max_length,max_length)
    height=min(mask.shape[0],max_length)
    width=min(mask.shape[1],max_length)
    padding_matrixs[:height,:width]=mask[:height,:width]
    return padding_matrixs

# 文本生成测试代码
def generate_sentence(content, model, tokenizer):
    model.eval()
    content= tokenizer.encode(content)
    with torch.no_grad():
        #生成了换行符，或生成文本超过30字则终止迭代
        while len(content) <= 120:
            x = torch.LongTensor([content])
            if torch.cuda.is_available():
                x = x.cuda()
            y = model(x)[0][-1]#y->sen_len,vocab_size
            index = sampling_strategy(y)
            content.append(index)
    return tokenizer.decode(content)


def sampling_strategy(prob_distribution):
    if random.random() > 0.1:
        strategy = "greedy"
    else:
        strategy = "sampling"
    if strategy == "greedy":
        return int(torch.argmax(prob_distribution))
    elif strategy == "sampling":
        prob_distribution = prob_distribution.cpu().numpy()
        return np.random.choice(list(range(len(prob_distribution))), p=prob_distribution)
#计算文本ppl





def train(corpus_path, save_weight):
    epoch_num = 20       #训练轮数
    batch_size = 32      #每次训练样本个数
    train_sample = 1000   #每轮训练总共训练的样本总数
    learning_rate = 0.001
    max_length = 120
    bert_path = r"C:\Users\晋晨曦\PycharmProjects\nlp_codes\week06\bert-base-chinese"

    model = LanguageModel(bert_path)    #建立模型
    tokenizer=BertTokenizer.from_pretrained(bert_path)
    corpus = load_corpus(corpus_path,tokenizer)     #加载语料
    if torch.cuda.is_available():
        model = model.cuda()
    optim = torch.optim.Adam(model.parameters(), lr=learning_rate)   #建立优化器
    train_dataset = build_dataset(batch_size,corpus,model,tokenizer,max_length)
    print("文本词表模型加载完毕，开始训练")
    for epoch in range(epoch_num):
        model.train()
        watch_loss = []
        for x,y,mask in train_dataset:
            # print(train_dataset)
            if torch.cuda.is_available():
                x,y,mask = x.cuda(), y.cuda(),mask.cuda()
            optim.zero_grad()    #梯度归零
            loss = model(x, y,mask)   #计算loss
            loss.backward()      #计算梯度
            optim.step()         #更新权重
            watch_loss.append(loss.item())
        print("=========\n第%d轮平均loss:%f" % (epoch + 1, np.mean(watch_loss)))
        print(generate_sentence("阿根廷歹徒抢服装尺码不对拿回店里换", model, tokenizer))
        print(generate_sentence("6月1日起北京实施“史上最严控烟期", model, tokenizer))
    if not save_weight:
        return
    else:
        base_name = os.path.basename(corpus_path).replace("txt", "pth")
        torch.save(model.state_dict(), base_name)
        return

if __name__ == "__main__":
    train("../sample_data.json", False)
